hive::
------
There are 3 server components of hive::
1.hive metastore
//It is used to store the table metadata.
2.hiveserver2
3.WebHcatServer

To get into the hive shell::
type hive and hit enter

to execute the command without entering into the hive shell::
hive -e "set;"

to search for the specific options in the set command use grep in combination with set;

hive -e "set;" | grep warehouse

set mapred.job.tracker=local //will set whether the job is running on local mode or mapreduce mode.
set hive.cli.print.current.db=true; gives the current db we are using by default it gives the default database.

All the table metadata is stored in metastore db which is configured in the parameters::(hive-site.xml)
javax.jdo.option.connectionURL which is pointing towards the RDBMS.

//creation of a sample database in hive::
create database nyse;

//use database
use nyse;


//There are several types of datatypes are available while creating the columns in hive::
tinyint,bigint,smallint,int,float,double,string,varchar,char,timestamp(unix related timestamp),Date

//There are also complex datatypes allowed::
arrays,maps,structs,unions


//creation of a sample table in hive::
create table t_old( i int,s string);

alter table t_old rename to t_new;


//insert into the table
insert into table t values(1,"HelloWorld")

you can view the table using the following commands::
describe t;
//will show the table columns along with the datatypes.

describe extended t;
//shows some extra information

describe formatted t;
//all the metadata of the table in readable format is displayed.
//such as inputformat,outputformat,field delimiters and line delimiters,owner,created date,number of rows

set hive.metastore.warehouse.dir;
//will display where the data is stored.

//whenever any operations are performed in the hive tables these operations will trigger an MR jobs.

//The hive logs will be stored in the /tmp directory since it uses the directory (java.io.tmp) as default.under this directory it will have the name of that particular user where it will store the hive logs on the daily basis.. the current hive log will be stored as hive.log file.

//If we want to overrite the specific functions we can override the configurations in hiverc file.
It will be present in the ~/.hivrc

you can set the properties such as hive.cli.print.current.db=true;
//will show the db currently using...

//There are 2 types of tables in hive::
1.external table //It is used when the data already exists in the hdfs directory and there is no corresponding table for that.
2.managed table //It is used to create when the data doesnot exists in the hdfs directory its like creating a normal table.

To create an external table::
//specifying the location for external table is mandatory since it depends on the location.

create external table nyse_external_table(
stockticker string,
tradedate int,
openprice float,
highprice float,
lowprice float,
closeprice float,
volume bigint)
row format delimited fields terminated by ','
stored as textfile
location '/user/edureka/nyse_external'

To create an managed table::
//specifying the location for the managed table is optional.

create table nyse_managed_table(
stockticker string,
tradedate int,
openprice float,
highprice float,
lowprice float,
closeprice float,
volume bigint)
row format delimited fields terminated by ','
stored as textfile
location '/user/edureka/nyse_managed'

when we drop the both of the tables using drop command as follows::
drop table nyse_managed_table;
drop table nyse_external_table;

//the external data will not be deleted in the hdfs, the reason is it will be retained for running some other alternate programming paradigms such as mapreduce,spark jobs.

//when we drop the managed table it will delete the corresponding hdfs directory aswell.

//to get how the table is created use::
show create tahle t;

//to create the table and load the local data into the table::

create table nyse_table(
stockticker string,
tradedate int,
openprice float,
highprice float,
lowprice float,
closeprice float,
volume bigint)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile


load data local inpath '/home/edureka/KIRANWS/Datasets/nyse/nyse_all' into table nyse_table

//to load the data that already exists in the hdfs use::

create table nyse_hdfs_table(
stockticker string,
tradedate int,
openprice float,
highprice float,
lowprice float,
closeprice float,
volume bigint)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile

load data inpath 'hdfs://localhost:8020/user/edureka/nyse_year/" into table nyse_hdfs_table

//If the data for the table is loaded from hdfs directory in to the table then the contents from the source hdfs directory are moved to the destination directory (thus it increases the peformance of the hdfs).

//We can set the partitions for the hive table in 2 ways::

1.list (partitioned by)
2.Hash (clustered by)

you can create the partition table using ::

create table nyse_list_table(
stockticker string,
tradedate int,
openprice float,
highprice float,
lowprice float,
closeprice float,
volume bigint)
partitioned by(tradeyear int)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile

//describe formatted nyse_list_table
It will show the partitioned by column also in the table description.

//The partitioned column should not be part of the columns list specified as part of the table.

//The partitioned by will create the subdirectories in the hdfs directory of the corresponding hive table however these subdirectories are not available once the table is created it will be available only once if the partition is done.

//to add the partition::
alter table nyse_list_table add partition (tradeyear=2009);
alter table nyse_list_table add partition (tradeyear=2010);
alter table nyse_list_table add partition (tradeyear=2011);
alter table nyse_list_table add partition (tradeyear=2012);
alter table nyse_list_table add partition (tradeyear=2013);
alter table nyse_list_table add partition (tradeyear=2014);


 dfs -ls /user/hive/warehouse/nyse.db/nyse_list_table;                                                                    
Found 6 items
drwxr-xr-x   - edureka supergroup          0 2017-12-31 17:54 /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2009
drwxr-xr-x   - edureka supergroup          0 2017-12-31 17:59 /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2010
drwxr-xr-x   - edureka supergroup          0 2017-12-31 17:59 /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2011
drwxr-xr-x   - edureka supergroup          0 2017-12-31 17:59 /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2012
drwxr-xr-x   - edureka supergroup          0 2017-12-31 17:59 /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2013
drwxr-xr-x   - edureka supergroup          0 2017-12-31 17:59 /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2014
hive> 

we can load the data into these corresponding direcotories either by using the::
1.hadoop fs -put

hadoop fs -put /home/edureka/KIRANWS/Datasets/nyse/nyse_year/nyse_2009.txt /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2009/

2.load command

load data local inpath '/home/edureka/KIRANWS/Datasets/nyse/nyse_year/nyse_2010.txt' into table nyse_list_table 
partition(tradeyear=2010);

load data local inpath '/home/edureka/KIRANWS/Datasets/nyse/nyse_year/nyse_2011.txt' into table nyse_list_table 
partition(tradeyear=2011);

load data local inpath '/home/edureka/KIRANWS/Datasets/nyse/nyse_year/nyse_2012.txt' into table nyse_list_table 
partition(tradeyear=2012);

load data local inpath '/home/edureka/KIRANWS/Datasets/nyse/nyse_year/nyse_2013.txt' into table nyse_list_table 
partition(tradeyear=2013);

load data local inpath '/home/edureka/KIRANWS/Datasets/nyse/nyse_year/nyse_2014.txt' into table nyse_list_table 
partition(tradeyear=2014);


//when we select the table with the select command it will show the records of the table along with the partitioned column in the last however in hdfs it will store only the specified fields while creating the table, this extra field will come from the partition command which it internally created the directory.

select * from nyse_list_table limit 5;                                                                          
OK
ZTR	20091231	15.68	15.72	15.68	15.72	349600	2009
ZQK	20091231	2.07	2.07	2.07	2.07	455900	2009
ZNH	20091231	15.63	15.66	15.63	15.66	48800	2009
ZMH	20091231	59.65	59.89	59.65	59.89	562200	2009
ZLC	20091231	2.91	2.91	2.91	2.91	1412700	2009
Time taken: 0.131 seconds, Fetched: 5 row(s)


dfs -tail /user/hive/warehouse/nyse.db/nyse_list_table/tradeyear=2010/nyse_2010.txt
    > ;
8.05,0
AEC,20100101,11.27,11.27,11.27,11.27,0
AEB,20100101,17.05,17.05,17.05,17.05,0
ADX,20100101,10.1,10.1,10.1,10.1,0
ADS,20100101,64.59,64.59,64.59,64.59,0
ADM,20100101,31.31,31.31,31.31,31.31,0
ADC,20100101,23.29,23.29,23.29,23.29,0
ACO,20100101,28.42,28.42,28.42,28.42,0


//suppose, If there is a partition with 2015 with few records such as and loaded into the partition::

AEC,20150101,11.27,11.27,11.27,11.27,0
AEB,20150101,17.05,17.05,17.05,17.05,0
ADX,20150101,10.1,10.1,10.1,10.1,0
ADS,20150101,64.59,64.59,64.59,64.59,0
ADM,20150101,31.31,31.31,31.31,31.31,0
ADC,20150101,23.29,23.29,23.29,23.29,0
ACO,20150101,28.42,28.42,28.42,28.42,0

and we have the sample data for 2016 partition records in the local directory.

AEC,20160101,11.27,11.27,11.27,11.27,0
AEB,20160101,17.05,17.05,17.05,17.05,0
ADX,20160101,10.1,10.1,10.1,10.1,0
ADS,20160101,64.59,64.59,64.59,64.59,0
ADM,20160101,31.31,31.31,31.31,31.31,0
ADC,20160101,23.29,23.29,23.29,23.29,0
ACO,20160101,28.42,28.42,28.42,28.42,0

and if data(2016) is loaded in to the partition of 2015 then if the select command is applied it will treat that entire partition as 2015.

//the result will be::

select * from nyse_list_table where tradeyear=2015
    > ;
OK
AEC	20150101	11.27	11.27	11.27	11.27	0	2015
AEB	20150101	17.05	17.05	17.05	17.05	0	2015
ADX	20150101	10.1	10.1	10.1	10.1	0	2015
ADS	20150101	64.59	64.59	64.59	64.59	0	2015
ADM	20150101	31.31	31.31	31.31	31.31	0	2015
ADC	20150101	23.29	23.29	23.29	23.29	0	2015
AEC	20160101	11.27	11.27	11.27	11.27	0	2015
AEB	20160101	17.05	17.05	17.05	17.05	0	2015
ADX	20160101	10.1	10.1	10.1	10.1	0	2015
ADS	20160101	64.59	64.59	64.59	64.59	0	2015
ADM	20160101	31.31	31.31	31.31	31.31	0	2015
ADC	20160101	23.29	23.29	23.29	23.29	0	2015
Time taken: 0.132 seconds, Fetched: 12 row(s)
hive> 

The key take aways for the list partition are ::
1.when the partitioned table is created the partitioned column should be independent of the table column.
2.when the wrong data is loaded into the partition it will not throw any errors.

For the table partitioning there can be 2 scenarios::

1.Having data independently & then creating the partition directory & loading.
2.having already unpartitioned data in the table which is to be stored in the partitions.(dynamic partition).
	a.table with the unpartitioned data.
	b.table with partitioned schema.
	
//Inorder to load the data into the table::
//we have to set the property
set	hive.exec.dynamic.partition.mode=nonstrict

insert into table nyse_list_table partition (tradeyear) 
select t.*,cast(substr(tradedate,1,4) as int) tradeyear from nyse_hdfs_table t;

//insert is always recommended than load because the load will be useful only if the preformatted data is available. If there are any transformations involved then it is better to load the data using the insert command.

//If we create a table with '|' as the delimiter and then load the data which is delimited by ',' and in if the select command is applied::
1.If the first data type of the table is string then it will show::
first field the content and the remaining fields as null.

ZTR,20091231,15.68,15.72,15.68,15.72,349600	NULL	NULL	NULL	NULL	NULL	NULL
ZQK,20091231,2.07,2.07,2.07,2.07,455900	NULL	NULL	NULL	NULL	NULL	NULL
ZNH,20091231,15.63,15.66,15.63,15.66,48800	NULL	NULL	NULL	NULL	NULL	NULL
ZMH,20091231,59.65,59.89,59.65,59.89,562200	NULL	NULL	NULL	NULL	NULL	NULL
ZLC,20091231,2.91,2.91,2.91,2.91,1412700	NULL	NULL	NULL	NULL	NULL	NULL
ZF,20091231,13.48,13.6,13.48,13.6,246400	NULL	NULL	NULL	NULL	NULL	NULL

2.If the first field is not of the string type then,

NULL	NULL	NULL	NULL
NULL	NULL	NULL	NULL
NULL	NULL	NULL	NULL
NULL	NULL	NULL	NULL
NULL	NULL	NULL	NULL

//The load into command will always append the contents to the table..
load data inpath 'directory' into table table_name

//to overwrite the table contents..
load data inpath 'directory' overwrite into table table_name

//you can execute a group of hive commands by inserting the commands into a file

cat > hive_sql.txt
use nyse;
select * from orders_stage limit 10;

hive -f hive_sql.txt 

//we can create a table of our own format and write the file of our own compression.

lets say we have created the table of sequencefile format.

create table orders_stage
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile;

create table orders_seq
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as sequencefile;

load data local inpath '/home/edureka/KIRANWS/Datasets/retail_db_comma/orders' into table orders_stage;

we have to inform the hive to use the compress and also inform what type of compression is used for that,

hive.exec.compress.output=true; //set for compression.
mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec; //setting the compression codec

to load the data into the orders_seq ::
insert into table order_seq
select * from orders_stage;

It will store with that compression specified and with that file format.

when we again set the hive.exec.compress.output=false;

and then again load the data using::

insert into table order_seq
select * from orders_stage;

It will show all the records of the table with out any error (since the file is in line with the file format specified it will show all the records).


Hash partitioning::(clustered by)::
-----------------------------------

The hash partitioning will be applied on the hash key.

While creating the table we need to specify the hash column to perform bucketing.

create table orders_bucket
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
clustered by (order_id) into 16 buckets
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile

//If the clustered by column is of a string type the hashcode of that particular string is calculated and inserted.

//Before inserting the data we have to set the property::
hive.enforce.bucketing=true;

//To load the data into the bucketing table we cannot use the load command so the insert command is used.

insert into orders_bucket
select * from orders_seq;

//The MR job will take care of the sequence file compressed data to be represented in the text file format.

//The hash partitioning is very useful when the retrieval of the data is faster and there will be the uniform distribution of the records.

//If you want to copy the result of the query to the local file use the query::

insert overwrite local directory '/home/edureka/orders_by_status' select order_status,count(1) from orders_stage group by order_status;

//The overwrite should be always defined while throwing the result.


HIVE QL::
---------

//For some of the select queries it will not trigger any MR jobs.

Ex:select * from categories limit 5;

//It will trigger only when there are any transformations are involved.

//while writing queries instead of using or condition we can use the in condition clause.

select count(1) from orders where order_status='Complete' or order_status='Closed';

select count(1) from orders where order_status in ('Complete','Closed');

In hive there are lot of functions available::
1.predefined
2.user defined

to show all the functions use::
show functions;

to get the definition of what does function do::
describe function max;


hive query to get the top 10 products which are sold by quantity where the transaction status of the order is either complete or closed

select * from (
select d.department_name,c.category_name,p.product_name,sum(oi.order_item_quantity) as totalquantity from order_items oi
inner join products p
on oi.order_item_product_id=p.product_id
inner join categories c
on c.category_id=p.product_category_id
inner join departments d
on d.department_id=c.category_department_id
inner join orders o
on o.order_id=oi.order_item_order_id
where o.order_status in ('COMPLETE','CLOSED')
group by oi.order_item_product_id,p.product_name,c.category_name,d.department_name 
order by totalquantity desc
) t limit 10;

to get the execution plan for  the query ::

explain query;

//It will return the execution plan of the query in stages.




Configurations::
1.hive.cli.print.current.db=true; //to print current db
2.hive.cli.print.headers=true;   //to print the table headers
3.hive.metastore.warehouse.dir;  //the location where the hive tables are stored.
4.hive.exec.dynamic.partition.mode=nonstrict  //insert into the dynamically created partition.
5.hive.exec.compress.output=true; //set for compression.
6.mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec; //setting the compression codec
7.hive.enforce.bucketing //to enforce bucketing(clustered by)
8.hive.execution.engine=mr;


doubts::
1.what is the use of invalidate metadata
2.why for some of the files the compression file is larger than the normal file
3.clusetered by vs partitioned by
4.load data into the corresponding hdfs directory and then create the table using hive and use the query select *



-------------
sample queries
-------------

create database kiran_stage;

use kiran_stage;

create table categories
(
category_id int,
category_department_id int,
category_name string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile;

load data local inpath '/home/edureka/KIRANWS/Datasets/retail_db_comma/categories/'
into table categories;

create external table customers
(
customer_id int,
customer_fname string,
customer_lname string,
customer_email string,
customer_password string,
customer_street string,
customer_city string,
customer_state string,
customer_zipcode string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/edureka/retail_db_comma/customers/';

create external table departments
(
department_id int,
department_name string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/edureka/retail_db_comma/departments/';


create external table order_items
(
order_item_id int,
order_item_order_id int,
order_item_product_id int,
order_item_quantity int,
order_item_subtotal float,
order_item_product_price float)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/edureka/retail_db_comma/order_items/';


create external table orders
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/edureka/retail_db_comma/orders/';


create external table products
(
product_id int,
product_category_id int,
product_name string,
product_description string,
product_price float,
product_image string)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile
location '/user/edureka/retail_db_comma/products/';


--------------------------

create database kiran_final;

use kiran_final;


create table categories
(
category_id int,
category_department_id int,
category_name string)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;


create table customers
(
customer_id int,
customer_fname string,
customer_lname string,
customer_email string,
customer_password string,
customer_street string,
customer_city string,
customer_state string,
customer_zipcode string)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;


create table departments
(
department_id int,
department_name string)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;


create table order_items
(
order_item_id int,
order_item_order_id int,
order_item_product_id int,
order_item_quantity int,
order_item_subtotal float,
order_item_product_price float)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;


create table orders
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;



create table products
(
product_id int,
product_category_id int,
product_name string,
product_description string,
product_price float,
product_image string)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;



set hive.exec.compress.output=true;
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec;


insert into table categories
select * from kiran_stage.categories;

insert into table customers
select * from kiran_stage.customers;

insert into table departments
select * from kiran_stage.departments;

insert into table order_items
select * from kiran_stage.order_items;

insert into table orders
select * from kiran_stage.orders;

insert into table products
select * from kiran_stage.products;

create table order_items_bucket
(
order_item_id int,
order_item_order_id int,
order_item_product_id int,
order_item_quantity int,
order_item_subtotal float,
order_item_product_price float)
clustered by (order_item_order_id) into 16 buckets
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;



create table orders_bucket
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
clustered by (order_id) into 16 buckets
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;

set hive.enforce.bucketing=true;


insert into table order_items_bucket
select * from order_items;

insert into table orders_bucket
select * from orders;

set hive.enforce.bucketing=false;


create table orders_partition
(
order_id int,
order_date string,
order_customer_id int,
order_status string)
partitioned by (order_month string)
row format delimited fields terminated by '|'
lines terminated by '\n'
stored as textfile;

set hive.exec.dynamic.partition.mode=nonstrict;

insert into table orders_partition partition(order_month)
select t.*,substr(t.order_date,6,2) month from orders t;

set hive.exec.compress.output=false;
set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec;
set hive.exec.dynamic.partitin.mode=strict;








 











